{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708f62c5-0fd5-41b0-9f90-8adf71d71a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8191b98d-b3b6-42ed-a6df-4ee903117d15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T01:29:43.392634Z",
     "start_time": "2024-04-14T01:29:39.910592Z"
    }
   },
   "outputs": [],
   "source": [
    "#Start finbert\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer_finbert = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer_finbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f421e57-1d28-47b3-ba87-a54595e6bc17",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-04-14T01:29:43.934225Z",
     "start_time": "2024-04-14T01:29:43.391507Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nThe new behaviour of LlamaTokenizer (with `self.legacy = False`) requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m base_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNousResearch/Llama-2-13b-hf\u001B[39m\u001B[38;5;124m\"\u001B[39m \n\u001B[1;32m      6\u001B[0m peft_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFinGPT/fingpt-sentiment_llama2-13b_lora\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 7\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mLlamaTokenizerFast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39meos_token\n\u001B[1;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m LlamaForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(base_model, trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, device_map \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m, load_in_8bit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,)\n",
      "File \u001B[0;32m~/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2048\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2045\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2046\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2048\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2049\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2050\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2051\u001B[0m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2052\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2053\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2054\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2055\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2056\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2057\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_is_local\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2058\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2059\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2060\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2082\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2080\u001B[0m has_tokenizer_file \u001B[38;5;241m=\u001B[39m resolved_vocab_files\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2081\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (from_slow \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_tokenizer_file) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mslow_tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 2082\u001B[0m     slow_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mslow_tokenizer_class\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2083\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2084\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2085\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2086\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2087\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2088\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2089\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2090\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_commit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2091\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2092\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2093\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2094\u001B[0m     slow_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2287\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   2285\u001B[0m \u001B[38;5;66;03m# Instantiate the tokenizer.\u001B[39;00m\n\u001B[1;32m   2286\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2287\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2288\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m   2289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m   2290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load vocabulary from file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2291\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2292\u001B[0m     )\n",
      "File \u001B[0;32m~/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py:182\u001B[0m, in \u001B[0;36mLlamaTokenizer.__init__\u001B[0;34m(self, vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs, add_bos_token, add_eos_token, clean_up_tokenization_spaces, use_default_system_prompt, spaces_between_special_tokens, legacy, add_prefix_space, **kwargs)\u001B[0m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_eos_token \u001B[38;5;241m=\u001B[39m add_eos_token\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_default_system_prompt \u001B[38;5;241m=\u001B[39m use_default_system_prompt\n\u001B[0;32m--> 182\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msp_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_spm_processor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrom_slow\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_prefix_space \u001B[38;5;241m=\u001B[39m add_prefix_space\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    186\u001B[0m     bos_token\u001B[38;5;241m=\u001B[39mbos_token,\n\u001B[1;32m    187\u001B[0m     eos_token\u001B[38;5;241m=\u001B[39meos_token,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    199\u001B[0m )\n",
      "File \u001B[0;32m~/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama.py:214\u001B[0m, in \u001B[0;36mLlamaTokenizer.get_spm_processor\u001B[0;34m(self, from_slow)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    213\u001B[0m     sp_model \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m--> 214\u001B[0m     model_pb2 \u001B[38;5;241m=\u001B[39m \u001B[43mimport_protobuf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mThe new behaviour of \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__class__\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m (with `self.legacy = False`)\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m     model \u001B[38;5;241m=\u001B[39m model_pb2\u001B[38;5;241m.\u001B[39mModelProto\u001B[38;5;241m.\u001B[39mFromString(sp_model)\n\u001B[1;32m    216\u001B[0m     normalizer_spec \u001B[38;5;241m=\u001B[39m model_pb2\u001B[38;5;241m.\u001B[39mNormalizerSpec()\n",
      "File \u001B[0;32m~/hakaton-gagarin-sentiment_interface/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:43\u001B[0m, in \u001B[0;36mimport_protobuf\u001B[0;34m(error_message)\u001B[0m\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sentencepiece_model_pb2\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 43\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(PROTOBUF_IMPORT_ERROR\u001B[38;5;241m.\u001B[39mformat(error_message))\n",
      "\u001B[0;31mImportError\u001B[0m: \nThe new behaviour of LlamaTokenizer (with `self.legacy = False`) requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "#Start FinGPT\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from peft import PeftModel  # 0.5.0\n",
    "\n",
    "base_model = \"NousResearch/Llama-2-13b-hf\" \n",
    "peft_model = \"FinGPT/fingpt-sentiment_llama2-13b_lora\"\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = LlamaForCausalLM.from_pretrained(base_model, trust_remote_code=True, device_map = \"cuda:0\", load_in_8bit = True,)\n",
    "model = PeftModel.from_pretrained(model, peft_model)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257da82b-1657-4d45-8c99-83d4d6d03bbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T01:29:47.030130Z",
     "start_time": "2024-04-14T01:29:46.804215Z"
    }
   },
   "outputs": [],
   "source": [
    "#Start Other \n",
    "import translators as ts\n",
    "def translate(message):\n",
    "    try:\n",
    "        message = ts.translate_text(message, translator='google', from_language='ru', to_language='en')\n",
    "        return (True, message)\n",
    "    except Exception:\n",
    "        return (False, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e79c5b-31de-4b4f-bcc6-a13f3cf16db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T01:29:47.843486Z",
     "start_time": "2024-04-14T01:29:47.838332Z"
    }
   },
   "outputs": [],
   "source": [
    "#finbert func\n",
    "def finbert(text):\n",
    "    state, text = translate(text)\n",
    "    if state == 0:\n",
    "        return 0\n",
    "        \n",
    "    sentences = [text]\n",
    "    label = {\n",
    "        \"Negative\": 1,\n",
    "        \"Neutral\": 3,\n",
    "        \"Positive\" : 5\n",
    "        \n",
    "    }\n",
    "    results = nlp(sentences)\n",
    "    return label[results[0]['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bf7fa1-aa34-42e8-bc81-be9cc010fe82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T01:29:48.600191Z",
     "start_time": "2024-04-14T01:29:48.593042Z"
    }
   },
   "outputs": [],
   "source": [
    "#FinGPT func\n",
    "def fingpt(text):\n",
    "    state, text = translate(text)\n",
    "    text = '''Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
    "        Input: '''+text+'''\n",
    "        Answer: '''\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding='max_length', max_length=512)\n",
    "    res = model.generate(**tokens, max_length=512)\n",
    "    res_sentences = [tokenizer.decode(i) for i in res]\n",
    "    out_text = [o.split(\"Answer: \")[1] for o in res_sentences]\n",
    "    out_text = out_text[0]\n",
    "    if out_text.find(\"negative\") != -1:\n",
    "        res = 1\n",
    "    elif out_text.find(\"positive\") != -1:\n",
    "        res = 5\n",
    "    elif out_text.find(\"neutral\") != -1:\n",
    "        res = 3\n",
    "    else:\n",
    "        res = 0\n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b541b4d-ec12-4fae-b5f7-d57f7f565a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T01:29:49.482107Z",
     "start_time": "2024-04-14T01:29:49.478721Z"
    }
   },
   "outputs": [],
   "source": [
    "#Random func\n",
    "def random_gpt(text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f79f494-55d6-471b-abee-ad502413f907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T01:29:50.285789Z",
     "start_time": "2024-04-14T01:29:50.278737Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_news(news):\n",
    "    res_1 = fingpt(news)\n",
    "    res_2 = finbert(news)\n",
    "    res_3 = random_gpt(news)\n",
    "    #res = (res_1+res_2)/2 #Бред, там есть 0\n",
    "    print(res_1, res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b96a31f-8e85-4731-a266-aa23087ae442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T01:29:52.931938Z",
     "start_time": "2024-04-14T01:29:52.437541Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcheck_news\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mГазпром вырос на 15\u001B[39;49m\u001B[38;5;124;43m%\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m, in \u001B[0;36mcheck_news\u001B[0;34m(news)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_news\u001B[39m(news):\n\u001B[0;32m----> 2\u001B[0m     res_1 \u001B[38;5;241m=\u001B[39m \u001B[43mfingpt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnews\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     res_2 \u001B[38;5;241m=\u001B[39m finbert(news)\n\u001B[1;32m      4\u001B[0m     res_3 \u001B[38;5;241m=\u001B[39m random_gpt(news)\n",
      "Cell \u001B[0;32mIn[5], line 7\u001B[0m, in \u001B[0;36mfingpt\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m      3\u001B[0m state, text \u001B[38;5;241m=\u001B[39m translate(text)\n\u001B[1;32m      4\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'''\u001B[39m\u001B[38;5;124mInstruction: What is the sentiment of this news? Please choose an answer from \u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124mnegative/neutral/positive}\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124m    Input: \u001B[39m\u001B[38;5;124m'''\u001B[39m\u001B[38;5;241m+\u001B[39mtext\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124m    Answer: \u001B[39m\u001B[38;5;124m'''\u001B[39m\n\u001B[0;32m----> 7\u001B[0m tokens \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m(text, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m'\u001B[39m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m)\n\u001B[1;32m      8\u001B[0m res \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtokens, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m)\n\u001B[1;32m      9\u001B[0m res_sentences \u001B[38;5;241m=\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m res]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "check_news(\"Газпром вырос на 15%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b974b123-8c42-404d-b6ff-71fd952ecd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10a4ab-2005-40ae-86c0-a97ca8e2e9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
